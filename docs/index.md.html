**Scientific cluster**

Introduction
============

`Scientific cluster` provides a virtual simulation on a cluster based on
[LUSTRE](http://lustre.org) (a open-source parallel file system used in HPC) and [SLURM](https://slurm.schedmd.com) (a open-source fault-tolerant cluster
management system). This is arguably the most common architecture used in
scientific computer clusters.

The virtual simulation runs on top of [QEMU](http://www.qemu.org).

The main purpose of this package is to allow administrators of scientific
clusters to test different configurations before deployment.

In the near future we will also support [Singularity](http://singularity.lbl.gov/index.html) (A user managed container system allowing the deployment of
complex workloads on a cluster).

There might be other components included (for example, OpenLDAP and NFS).

Requirements
============

- An OS that can run QEMU. This was tested on Linux as a host
  environment.  We suspect that it will almost "out of the box" on a
  Mac and it will need some work on Windows
- QEMU
- 12 GB of memory
- sshpass
- bash
- ~30 GB of free disk space
- root access

Optional (but strongly recommended):

- VNC client

Example Architecture
======================

![Cluster architecture](cluster.png)

!!! Note to self
    Explain the architecture


!!! WARNING
    The current architecture does not follow best practices.
	As you can see there are cases that have both server and clients
	of LUSTRE. Users can put jobs on LUSTRE and SLURM master nodes.
	This is one example of architecture. You can use this framework
	to test

This will include the following machines (host names given as definition)



## General admin

ldap
:   A support machine with a LDAP server for user account information.

DHCP will be supplied by QEMU as configured by us.

Name resolution will be hard-coded in all machines' `/etc/hosts`
files.

## Core

head
:   User/SLURM Head node with LUSTRE Client

slrm-master
:   SLURM master with user compute access and LUSTRE client

lustre-master
:   LUSTRE master ***and client***. User compute access

nfs
:   NFS export with LUSTRE client

## Researcher


Installation
=============


!!! WARNING
    Make sure you read the Architecture section above before
    proceeding. It includes all the machine names and descriptions
    that we will be creating.

!!!
    All work that you need todo will be preceded box like this. The
    rest is explanation.


## Creation of a base image


### Basic image initialization

The first issue is the creation of a base image that will be used to
create all other images:

The steps are:

- Download the ISO for Ubuntu Server 16.04
- Patch the ISO to auto-install
- Create a virtual disk
- Run the patched ISO to create the base image

!!!
    Run a script and connect to VNC

Run the script `scripts/create_base_image.sh`.

Note that it will create a
**password-less** VNC interface on *:1 .

This will take a lot of time to run. Connect to the VNC interface as
soon as it is up to monitor the progress. Kill the script as soon as there
machine reboots, after clicking continue here:

![Install finish](install_finish.png)

Wait for the machine to reboot and kill QEMU (do not let it boot).

The machine has SSH and the root password is test. We will take care of
security in step 2.

### Securing and backing up the base image

Now you have `pre-base.img`.


!!!
    Copy a image file

`cp base.img backup.img`

Copy this to `base.img` and `backup.img`.
The backup image will be useful if you have problems from now on.

The base image will be patched to allow public/private key login instead of
password.

!!!
    Start the base image with

```bash
qemu-system-x86_64 -m 1G -drive file=base.img,format=raw -vnc :1 \
  -net nic,macaddr=52:54:00:12:34:56 -net socket,listen=:1234 \
  -net user,hostfwd=tcp:127.0.0.1:22222-:22
```

Monitor by connecting to the VNC interface.

!!!
    Test the ssh login
    
`ssh -p22222 root@localhost`

The password is `test`. Note that the VM might take some time to start
(monitor its progress with VNC).

!!!
    Put you public key in the current directory

This will be `id_rsa.pub` or `id_dsa.pub`.

!!!
    Upload the public key

Run `scripts/prepare_security.sh`. Give it some time to halt (or check
on VNC). Shutdown QEMU.

!!! WARNING
    Security here is nothing special.
    This is for testing and prototyping configurations, not production

### Basic networking

Here we will configure the base image to accept DHCP hostnames and
add a `/etc/hosts` file with all host names preloaded.

!!!
   Run a script
   
The script is `scripts/prepare_networking.sh`

!!!
   Shutdown the machine
   
Login as root (port 22222) and halt the machine.

Shutdown QEMU.

### Sanity checking and backup

!!!
    Start the machine again

```bash
qemu-system-x86_64 -m 1G -drive file=base.img,format=raw -vnc :1 \
  -net nic,macaddr=52:54:00:12:34:56 -net socket,listen=:1234 \
  -net user,hostfwd=tcp:127.0.0.1:22222-:22,hostname=base.cluster
```

!!!
    Login and check if all is OK

`ssh -p22222 root@localhost`

- **No password** should be requested
- On the prompt the machine should be called base
- `hostname` should return `base.cluster`
- `cat /etc/hosts | grep 2.15` should report a ldap name
  The login will report `base` as host


!!!
    Halt and shutdown
	
As with previous shutdown cases, make sure the machine really shuts
down before killing QEMU.



## Other stuff

singularity

(add bioconda also?)

Making your configurations
=============================


FAQ
===

## Why not Docker?

LUSTRE requires access to block volumes.

## Why not KVM (Kernel-based Virtual Machine)?

It should be easy to alter the QEMU commands (`-enable-kvm`) for this.
But, to ensure compatibility on Windows and Mac, this is not active.


Contact
=======


<style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style>

<script src="https://casual-effects.com/markdeep/latest/markdeep.min.js"></script>

<script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
